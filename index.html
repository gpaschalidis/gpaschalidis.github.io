<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>George Paschalidis</title>
    <!-- Font Awesome for general icons -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" rel="stylesheet">
    <!-- Academicons for Google Scholar -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.1/css/academicons.min.css" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="main-container">
        <!-- Header with Navigation Bar -->
        <header>
            <nav aria-label="Main Navigation">
                <ul>
                    <li><a href="https://gpaschalidis.github.io/" class="site-name">Georgios Paschalidis</a></li>
                    <li><a href="#home" id="home-nav-button" class="nav-link">Home</a></li>
                    <li><a href="#publications" id="publications-nav-button" class="nav-link">Publications</a></li>
                    <li><a href="#contact" id="contact-nav-button" class="nav-link">Contact</a></li>
                </ul>
            </nav>
        </header>


        <section id="home">
        <!-- Personal information section -->
        <div class="profile-container">
            
            <div class="image-links-container">
                <img src="george.jpg" alt="Profile Image of Georgios Paschalidis" class="profile-image">
            
                <div class="social-links">
                    <a href="mailto:g.paschalidis@uva.nl" target="_blank" aria-label="Send email to g.paschalidis@uva.nl"><i class="fa fa-envelope fa-2x" aria-hidden="true"></i></a>
                    <a href="https://github.com/gpaschalidis" target="_blank" aria-label="Visit my GitHub profile"><i class="fa-brands fa-github fa-2x" aria-hidden="true"></i></a>
                    <a href="https://x.com/gkpaschalidis" target="_blank" aria-label="Visit my Twitter profile" ><i class="fa-brands fa-twitter fa-2x" aria-hidden="true"></i></a>
                    <a href="https://www.linkedin.com/in/george-paschalidis-59bb06264/" target="_blank" aria-label="Visit my LinkedIn profile"><i class="fab fa-linkedin fa-2x" aria-hidden="true"></i></a>
                    <a href="https://scholar.google.com/citations?user=dYhRJjwAAAAJ&hl=en" target="_blank" aria-label="Visit my Google Scholar profile"><i class="ai ai-google-scholar ai-2x" aria-hidden="true"></i></a>
                </div>
            <!-- You can add more links here if you want -->
            </div>

            <div class="biography-note">
            <p>I am an ELLIS PhD student in the <a href="https://ivi.fnwi.uva.nl/cv" target="_blank">Computer Vision Lab</a> at 
               the <a href="https://www.uva.nl" target="_blank">University of Amsterdam</a>,
               advised by <a href="https://dtzionas.com" target="_blank">Prof. Dimitris Tzionas</a>. My research focus on 3D Human 
               Object Interaction (HOI) synthesis, while I am also interested in reconstructing 4D HOIs from videos. Before 
               joining UvA I had the great opportunity to spend 4 months as a research intern at <a href="https://www.sfu.ca/" target="_blank">Simon Fraser University</a> 
               working together with <a href="https://msavva.github.io/" target="_blank">Prof. Manolis Savva</a>. Prior to that
               I completed my Master at the <a href="https://www.upatras.gr/en/" target="_blank">University of Patras</a> 
               collaborating with Prof. Emmanouil Psarakis, while also working as a Lead Quality Assurance Enginner 
               at <a href="https://www.haf.gr/en/" target="_blank">Hellenic Air Force</a>. I am also a passionate 
               windsurfer. However, when the sea and wind are not there I enjoy spending my time running or going to the gym.</p>
            </div>
        </div>
        </section>
        
        <section id="publications" class="publications-section">
            <div class="publications-content">
                <h2>Publications</h2>
                    <div class="publication">
                        <div class="image">
                            <img src="sdfit/sdfit_teaser.png" alt="" />
                        </div>
                        <div class="publication-info">
                            <div class="paper-title">
                                <a href="">SDFit: 3D Object Pose and Shape by Fitting a Morphable SDF to a Single Image</a>
                            </div>
                            <div class="conference">
                                International Conference on Computer Vision (ICCV), 2025
                            </div>
                            <div class="authors">
                                <a href="https://anticdimi.github.io/" class="author">Dimitrije Antić</a>, 
                                <strong class="author">Georgios Paschalidis</strong>, 
                                <a href="https://sha2nkt.github.io/" class="author">Shashank Tripathi</a>, 
                                <a href="https://staff.science.uva.nl/th.gevers/" class="author">Theo Gevers</a>, 
                                <a href="https://saidwivedi.in/" class="author">Sai KUmar Dwivedi</a>, 
                                <a href="https://dtzionas.com/" class="author">Dimitrios Tzionas</a>, 
                            </div>
                            <div class="links">
                                <a href="#" data-type="Abstract" data-index="0">Abstract</a> 
                                <a href="" data-type="Project page">Project page</a> 
                                <a href="https://arxiv.org/abs/2409.16178" data-type="Paper">Paper</a> 
                                <a href="" data-type="Code">Code</a> 
                                <a href="" data-type="Video">Video</a> 
                                <a href="#" data-type="Bibtex" data-index="5">Bibtex</a>
                                <div class="link-content" data-index="0">
                                    <pre>
                                        Recovering 3D object pose and shape from a single image is a challenging and
                                        highly ill-posed problem. This is due to strong (self-)occlusions, depth
                                        ambiguities, the vast intra- and inter-class shape variance, and lack of 3D
                                        ground truth for natural images.  While existing methods train deep networks on
                                        synthetic datasets to predict 3D shapes, they often struggle to generalize to
                                        real-world scenarios, lack an explicit feedback loop for refining noisy
                                        estimates, and primarily focus on geometry without explicitly considering pixel
                                        alignment. To this end, we make two key observations: (1) a robust solution
                                        requires a model that imposes a strong category-specific shape prior to
                                        constrain the search space, and (2) foundational models embed 2D images and 3D
                                        shapes in joint spaces; both help resolve ambiguities.  Hence, we propose SDFit,
                                        a novel optimization framework that is built on three key innovations: First, we
                                        use a learned morphable signed-distance-function (mSDF) model that acts as a
                                        strong shape prior, thus constraining the shape space. Second, we use
                                        foundational models to establish rich 2D-to-3D correspondences between image
                                        features and the mSDF. Third, we develop a fitting pipeline that iteratively
                                        refines both shape and pose, aligning the mSDF to the image.  We evaluate SDFit
                                        on the Pix3D, Pascal3D+, and COMIC image datasets.  SDFit performs on par with
                                        SotA methods, while demonstrating exceptional robustness to occlusions and
                                        requiring no retraining for unseen images. Therefore, SDFit contributes new
                                        insights for generalizing in the wild, paving the way for future research. Code
                                        will be released.
                                    </pre>
                                </div>
                                <div class="link-content" data-index="5">
                                    <pre>
                                        @article{antic2024sdfit,
                                         title={{SDFit}: {3D} {O}bject Pose and Shape by Fitting a Morphable {SDF} to a Single Image},
                                         author={Dimitrije Antić and Sai Kumar Dwivedi and Shashank Tripathi and Theo Gevers and Dimitrios Tzionas},
                                          journal={{arXiv}:2409.16178},
                                          year={2024}
                                        }
                                    </pre>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="publication">
                        <div class="image">
                            <img src="cwgrasp/images/teaser.png" alt="3D Whole-Body Grasp Synthesis with Directional Controllability" />
                        </div>
                        <div class="publication-info">
                            <div class="paper-title">
                                <a href="http://gpaschalidis.github.io/cwgrasp">3D Whole-Body Grasp Synthesis with Directional Controllability</a>
                            </div>
                            <div class="conference">
                                International Conference on 3D Vision (3DV), 2025
                            </div>
                            <div class="authors">
                                <strong class="author">Georgios Paschalidis</strong>, 
                                <a href="https://www.linkedin.com/in/romana-wilschut-766381197/" class="author">Romana Wilschut</a>, 
                                <a href="https://anticdimi.github.io/" class="author">Dimitrije Antić</a>, 
                                <a href="https://otaheri.github.io/" class="author">Omid Taheri</a>, 
                                <a href="https://dtzionas.com/" class="author">Dimitrios Tzionas</a>, 
                            </div>
                            <div class="links">
                                <a href="#" data-type="Abstract" data-index="0">Abstract</a> 
                                <a href="https://gpaschalidis.github.io/cwgrasp" data-type="Project page">Project page</a> 
                                <a href="https://arxiv.org/abs/2408.16770" data-type="Paper">Paper</a> 
                                <a href="https://github.com/gpaschalidis/CWGrasp" data-type="Code">Code</a> 
                                <a href="https://www.youtube.com/watch?v=d9a4C2GHIv0" data-type="Video">Video</a> 
                                <a href="#" data-type="Bibtex" data-index="5">Bibtex</a>
                                <div class="link-content" data-index="0">
                                    <pre>
                                        Synthesizing 3D whole bodies that realistically grasp objects is useful for
                                        animation, mixed reality, and robotics. This is challenging, because the hands
                                        and body need to look natural w.r.t. each other, the grasped object, as well as
                                        the local scene (i.e., a receptacle supporting the object). Moreover, training
                                        data for this task is really scarce, while capturing new data is expensive.
                                        Recent work goes beyond finite datasets via a divide-and-conquer approach; it
                                        first generates a “guiding” right-hand grasp, and then searches for bodies that
                                        match this. However, the guiding-hand synthesis lacks controllability and
                                        receptacle awareness, so it likely has an implausible direction (i.e., a body
                                        can’t match this without penetrating the receptacle) and needs corrections
                                        through major post-processing. Moreover, the body search needs exhaustive
                                        sampling and is expensive. These are strong limitations. We tackle these with a
                                        novel method called CWGrasp. Our key idea is that performing geometry-based
                                        reasoning “early on,” instead of “too late,” provides rich “control” signals
                                        for inference. To this end, CWGrasp first samples a plausible
                                        reaching-direction vector (used later for both the arm and hand) from a
                                        probabilistic model built via ray-casting from the object and collision
                                        checking. Then, it generates a reaching body with a desired arm direction, as
                                        well as a “guiding” grasping hand with a desired palm direction that complies
                                        with the arm’s one. Eventually, CWGrasp refines the body to match the “guiding”
                                        hand, while plausibly contacting the scene. Notably, generating
                                        already-compatible “parts” greatly simplifies the “whole”. Moreover, CWGrasp
                                        uniquely tackles both right and left-hand grasps. We evaluate on the GRAB and
                                        ReplicaGrasp datasets. CWGrasp outperforms baselines, at lower runtime and
                                        budget, while all components help performance. Code and models are available for
                                        for research.
                                    </pre>
                                </div>
                                <div class="link-content" data-index="5">
                                    <pre>
                                        @inproceedings{paschalidis2025cwgrasp,
                                        title={{3D} {W}hole-Body Grasp Synthesis with Directional Controllability},
                                        author={Paschalidis, Georgios and Wilschut, Romana and Anti\'{c}, Dimitrije and Taheri, Omid and Tzionas, Dimitrios},
                                        booktitle = {{International Conference on 3D Vision (3DV)}},
                                        year={2025}
                                        }
                                    </pre>
                                </div>
                            </div>
                        </div>
                    </div>
        

            </div>
        </section>

        <section id="contact" class="contact-section">
            <h2>Contact</h2>
                <div class="contact-info">
                    <!-- Envelope Icon using Font Awesome -->
                    <div class="envelope-icon">
                        <a target="_blank"><i class="fa fa-envelope fa-2x" aria-hidden="true"></i>
                        </a>
                    </div>
                    <!-- Mail Link -->
                    <a href="mailto:g.paschalidis@uva.nl" class="mail-link">g.paschalidis@uva.nl</a>
                </div>
        </section>
    
<!--Java script for removing leading spaces and Tabs inside <pre> --> 
    <script> 
        document.querySelectorAll('.link-content pre').forEach(function(pre) {
            // Remove leading whitespace from each line of the <pre> content
            pre.innerHTML = pre.innerHTML
                .replace(/^\s+/g, '') // Remove leading whitespace on each line
                .replace(/\n\s+/g, '\n'); // Remove spaces after line breaks (if any)
        });
    </script>

<!--Java script for handling the contents of the Links Abstract and BibTex --> 
    <script type="text/javascript">
        document.querySelectorAll(".links").forEach(function (p) {
            p.addEventListener("click", function (ev) {
                if (ev.target.nodeName != "A") {
                    return;
                }
                var i = ev.target.dataset["index"];
                if (i == undefined) {
                    return;
                }
                Array.prototype.forEach.call(ev.target.parentNode.children, function (sibling) {
                    if (sibling.nodeName != "DIV") {
                        return;
                    }
                    if (sibling.dataset["index"] != i) {
                        sibling.style.display = "none";
                    } else {
                        sibling.style.display = sibling.style.display != "block" ? "block" : "none";
                    }
                });
                ev.preventDefault();
            });
        });
    </script>



    <script>
    document.addEventListener("DOMContentLoaded", function () {
    const sections = document.querySelectorAll("section");
    const navLinks = document.querySelectorAll(".nav-link");
    let currentSection = "home"; // Default to the Home section
    let isScrolling = false; // Flag to prevent conflicts during smooth scrolling
    let scrollTimeout; // Timeout to handle scroll completion

    // Get the header height once
    const headerHeight = document.querySelector('header').offsetHeight;

    // Initialize the active link
    navLinks.forEach((link) => link.classList.remove("active"));
    document.querySelector('.nav-link[href="#home"]').classList.add("active");
    // Smooth scrolling and updating the active link logic
    navLinks.forEach((link) => {
        link.addEventListener("click", (e) => {
            e.preventDefault(); // Prevent default anchor behavior
            const targetId = link.getAttribute("href").substring(1);
            const targetSection = document.getElementById(targetId);


            if (targetSection) { 
                // Prevent scroll conflict 
                isScrolling = true; 

                // Calculate the position considering the header offset
                const targetPosition = targetSection.offsetTop - headerHeight; 

                // Smooth scroll to the section (adjusted by headerHeight)
                window.scrollTo({ 
                    top: targetPosition, 
                    behavior: "smooth", 
                }); 

                // Temporarily set active link manually 

                navLinks.forEach((l) => l.classList.remove("active")); 
                link.classList.add("active"); 

                // Allow the scroll event to resume after a delay
                clearTimeout(scrollTimeout); 
                scrollTimeout = setTimeout(() => { 
                    isScrolling = false; 
                }, 800); // Ensure this matches or exceeds the smooth scroll duration 
            } 
        }); 
    });


    window.addEventListener("scroll", () => {
        if (isScrolling) return; // Prevent updates during smooth scrolling
        let newSection = "";
        // Handle scroll to top explicitly
        if (window.scrollY === 0) {
            newSection = "home";
        } else {
            // Iterate through sections to determine the active section
            sections.forEach((section) => {
                const sectionTop = section.offsetTop;
                const sectionHeight = section.offsetHeight;
                if (window.scrollY >= sectionTop - sectionHeight / 3 &&
                    window.scrollY < sectionTop + sectionHeight - sectionHeight / 3) {
                    newSection = section.getAttribute("id");
                }
            });
            // Special handling for the last section
            const lastSection = sections[sections.length - 1];
            if (window.innerHeight + window.scrollY >= document.body.offsetHeight - 10) {
                newSection = lastSection.getAttribute("id");
            }
        }
        // Update active link if the section changes
        if (newSection !== currentSection && newSection !== "") {
            navLinks.forEach((link) => link.classList.remove("active"));
            const newLink = document.querySelector(`.nav-link[href="#${newSection}"]`);
            if (newLink) newLink.classList.add("active");
            currentSection = newSection;
        }
    });
});
    </script>
</body>
</html>
